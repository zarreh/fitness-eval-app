# ── Local dev (copy to backend/.env) ─────────────────────────────────────────
#   make env          → copies this file to backend/.env
#   make backend      → starts FastAPI with --reload on port 8000
#
# ── Docker (docker-compose.yml reads backend/.env at runtime) ─────────────────
#   make docker-up           → build images and start all services
#   make docker-pull-model   → pull llama3.2 into Ollama (first run only)
#
# ── LLM provider ─────────────────────────────────────────────────────────────
# For local dev:    LLM_PROVIDER=ollama  (Ollama must be running locally)
# For production:   LLM_PROVIDER=openai  (set OPENAI_API_KEY below)
# In Docker:        OLLAMA_BASE_URL is overridden to http://ollama:11434
#                   by docker-compose.yml — no need to change it here.

LLM_PROVIDER=ollama
LLM_MODEL=llama3.2
OLLAMA_BASE_URL=http://localhost:11434
OPENAI_API_KEY=sk-placeholder
